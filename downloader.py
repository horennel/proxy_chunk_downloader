import os
import threading
import time
import argparse
import requests
import urllib3
from urllib.parse import urlparse
from rich.progress import Progress, BarColumn, TimeRemainingColumn, DownloadColumn, TransferSpeedColumn
from bark_python import BarkClient, CBCStrategy

# Bark æ¨é€é…ç½®
client = BarkClient(device_key=os.getenv('BARK_DEVICE_KEY'), api_url=os.getenv('BARK_URL'))
client.set_encryption(
    key=os.getenv('BARK_KEY'),
    iv=os.getenv('BARK_IV'),
    strategy_cls=CBCStrategy
)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class Downloader:
    def __init__(self, url, output, num_threads, use_proxy, proxies, max_retries=3, retry_wait=1.0, verify_ssl=True):
        self.url = url
        self.output = output
        self.num_threads = num_threads
        self.use_proxy = use_proxy
        self.verify_ssl = verify_ssl
        self.proxies = proxies
        self.max_retries = max_retries
        self.retry_wait = retry_wait
        self.temp_files = [f"{output}.part{i}" for i in range(num_threads)]

        # Chrome UA æ± ï¼ˆä¸åŒç‰ˆæœ¬/ç³»ç»Ÿï¼‰
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.6533.88 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.54 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.6422.141 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.6367.60 Safari/537.36",
        ]

    def get_file_size(self):
        try:
            headers = requests.head(self.url, verify=self.verify_ssl, timeout=(30, 30)).headers
            return int(headers.get('Content-Length', 0))
        except Exception as e:
            raise Exception(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {e}")

    def download_range_with_rich(self, start, end, part_index, proxy, progress, task_id, ua):
        temp_file = self.temp_files[part_index]
        expected_size = end - start + 1

        if os.path.exists(temp_file) and os.path.getsize(temp_file) == expected_size:
            progress.update(task_id, advance=expected_size)
            return

        attempt = 0
        while attempt <= self.max_retries:
            try:
                headers = {
                    'Range': f'bytes={start}-{end}',
                    'User-Agent': ua
                }
                mode = 'ab' if os.path.exists(temp_file) else 'wb'
                downloaded = os.path.getsize(temp_file) if os.path.exists(temp_file) else 0
                if downloaded > 0:
                    headers['Range'] = f'bytes={start + downloaded}-{end}'

                proxy_dict = {"http": proxy, "https": proxy} if proxy else None

                with requests.get(self.url, headers=headers, proxies=proxy_dict, stream=True,
                                  timeout=(30, 30), verify=self.verify_ssl) as r:
                    r.raise_for_status()
                    with open(temp_file, mode) as f:
                        progress.update(task_id, completed=downloaded)
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                progress.update(task_id, advance=len(chunk))
                return
            except Exception as e:
                attempt += 1
                print(f"âš ï¸ Thread-{part_index} å¤±è´¥å°è¯• {attempt}/{self.max_retries}: {e}")
                if attempt > self.max_retries:
                    print(f"âŒ Thread-{part_index} æœ€ç»ˆå¤±è´¥")
                    return
                time.sleep(self.retry_wait * 60)

    def merge_parts(self):
        print("\nğŸ“¦ æ­£åœ¨è¿›è¡Œåˆå¹¶æ“ä½œ,è¯·ç­‰å¾…......")
        with open(self.output, 'wb') as f_out:
            for temp_file in self.temp_files:
                with open(temp_file, 'rb') as f_in:
                    f_out.write(f_in.read())
        for temp_file in self.temp_files:
            os.remove(temp_file)
        print(f"âœ… åˆå¹¶å®Œæˆï¼š{self.output}")

    def load_failed_parts(self, file_size):
        part_size = file_size // self.num_threads
        parts_to_download = []

        for i in range(self.num_threads):
            start = i * part_size
            end = file_size - 1 if i == self.num_threads - 1 else (start + part_size - 1)
            expected_size = end - start + 1
            temp_file = self.temp_files[i]

            if not os.path.exists(temp_file) or os.path.getsize(temp_file) != expected_size:
                parts_to_download.append(i)

        return parts_to_download

    def test_proxy(self, proxy):
        try:
            r = requests.get("https://www.google.com/", proxies={"http": proxy, "https": proxy}, timeout=10)
            if r.status_code == 200:
                return True
        except:
            pass
        return False

    def start(self):
        file_size = self.get_file_size()
        if file_size == 0:
            raise Exception("æ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼ŒURLå¯èƒ½æ— æ•ˆæˆ–æœåŠ¡å™¨ä¸æ”¯æŒ Range è¯·æ±‚")

        part_size = file_size // self.num_threads
        parts_to_download = self.load_failed_parts(file_size)

        if not parts_to_download:
            print("ğŸ“‚ æ‰€æœ‰åˆ†æ®µå·²å®Œæˆï¼Œæ— éœ€é‡å¤ä¸‹è½½ï¼Œç›´æ¥åˆå¹¶...")
            self.merge_parts()
            return

        threads = []
        with Progress(
                "[progress.description]{task.description}",
                BarColumn(),
                "[progress.percentage]{task.percentage:>3.0f}%",
                DownloadColumn(),
                TransferSpeedColumn(),
                TimeRemainingColumn(),
        ) as progress:
            for i in parts_to_download:
                start = i * part_size
                end = file_size - 1 if i == self.num_threads - 1 else (start + part_size - 1)
                proxy = self.proxies[i % len(self.proxies)] if self.use_proxy else None

                if proxy and not self.test_proxy(proxy):
                    print(f"ğŸš« æ— æ•ˆä»£ç† Thread-{i}: {proxy}ï¼Œè·³è¿‡è¯¥çº¿ç¨‹")
                    continue

                ua = self.user_agents[i % len(self.user_agents)]  # ğŸ‘ˆ åˆ†é…å›ºå®š UA
                task_id = progress.add_task(f"Thread-{i}", total=end - start + 1)
                t = threading.Thread(
                    target=self.download_range_with_rich,
                    args=(start, end, i, proxy, progress, task_id, ua)
                )
                t.start()
                threads.append(t)

            for t in threads:
                t.join()

        remaining = self.load_failed_parts(file_size)
        if not remaining:
            self.merge_parts()
            client.send_notification(
                title="ğŸ†  æ–‡ä»¶ä¸‹è½½æˆåŠŸ",
                body=f"æ–‡ä»¶{self.output}ä¸‹è½½æˆåŠŸ!",
                sound="shake",
                icon=os.getenv('BARK_ICON')
            )
        else:
            print(f"â¹ï¸ ä¸‹è½½æœªå®Œæˆï¼Œè¿˜æœ‰ {len(remaining)} ä¸ªåˆ†æ®µæœªå®Œæˆï¼Œç¨åå¯é‡æ–°è¿è¡Œä»¥ç»§ç»­ä¸‹è½½")
            client.send_notification(
                title="âŒ  æ–‡ä»¶ä¸‹è½½å¤±è´¥",
                body=f"æ–‡ä»¶{self.output}ä¸‹è½½å¤±è´¥!\nè¿˜æœ‰ {len(remaining)} ä¸ªåˆ†æ®µæœªå®Œæˆ!",
                sound="shake",
                icon=os.getenv('BARK_ICON')
            )


def guess_file_name_from_url(url):
    parsed = urlparse(url)
    return os.path.basename(parsed.path) or "downloaded_file"


def main():
    parser = argparse.ArgumentParser(description="å¤šçº¿ç¨‹æ–­ç‚¹ç»­ä¼ ä¸‹è½½å™¨ï¼ˆæ”¯æŒä»£ç†/SSL/è‡ªåŠ¨é‡è¯•/è‡ªå®šä¹‰UAï¼‰")
    parser.add_argument("url", help="æ–‡ä»¶ä¸‹è½½åœ°å€")
    parser.add_argument("--p", action="store_true", help="å¯ç”¨ä»£ç†æ¨¡å¼")
    parser.add_argument("--n", type=int, default=6, help="çº¿ç¨‹æ•°é‡ï¼ˆé»˜è®¤6ï¼‰")
    parser.add_argument("--name", type=str, help="è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤æ ¹æ®URLæ¨æ–­ï¼‰")
    parser.add_argument("--v", action="store_true", help="ç¦ç”¨SSLéªŒè¯")
    args = parser.parse_args()

    url = args.url
    num_threads = args.n
    output_name = args.name or guess_file_name_from_url(url)
    use_proxy = args.p
    verify_ssl = not args.v

    proxy_pool = [
        "http://127.0.0.1:6890",
        "http://127.0.0.1:6891",
        "http://127.0.0.1:6892",
        "http://127.0.0.1:6893",
        "http://127.0.0.1:6894",
        "http://127.0.0.1:6895",
    ]

    downloader = Downloader(
        url=url,
        output=output_name,
        num_threads=num_threads,
        use_proxy=use_proxy,
        proxies=proxy_pool,
        max_retries=12,
        retry_wait=5,
        verify_ssl=verify_ssl
    )
    downloader.start()


if __name__ == "__main__":
    main()
